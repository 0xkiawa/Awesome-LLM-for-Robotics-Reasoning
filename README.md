

| Category   | Subcategory      | Models             | ArXiv Link              |
|----------------------|-------------------------------|----------------------------------------|------------------------------------|
| LLM     | Open sourced LLM    | BERT      | [devlin2019bert](https://arxiv.org/abs/1810.04805)  | 
|      |         | T5                 | [raffel2023exploring](https://arxiv.org/abs/1910.10683)  |
|      |         | LLaMA                | [touvron2023llama](https://arxiv.org/abs/2302.13971)  |
|      |         | OpenFlamingo                | [awadalla2023openflamingo](https://arxiv.org/abs/2308.01390)  |
|      |         | InstructBLIP                | [dai2023instructblip](https://arxiv.org/abs/2305.06500)  |
|      |         | ChatBridge                | [zhao2023chatbridge](https://arxiv.org/abs/2305.16103)  |
|      |  Closed sourced LLM   | GPT3                 | [brown2020language](https://arxiv.org/abs/2005.14165)  |
|      |         | GPT4                 | [openai2023gpt4](https://arxiv.org/abs/2303.08774)  |
|      | Instruction Turning   | InstructGPT              | [ouyang2022training](https://arxiv.org/abs/2203.02155)  |
|      |         | LLaVA                | [liu2023visual](https://arxiv.org/abs/2304.08485)   |
|      |         | MiniGPT-4               | [zhu2023minigpt4](https://arxiv.org/abs/2304.10592)   |
|      |         | FLAN                 | [wei2022finetuned](https://arxiv.org/abs/2109.01652)  |
|      |         | LLaMA-adapter              | [zhang2023llamaadapter](https://arxiv.org/abs/2303.16199) |
|      |         | Self-Instruct                 | [wang2023selfinstruct](https://arxiv.org/abs/2212.10560)  |
|      | Vision-LLM     | LLaVA                | [liu2023visual](https://arxiv.org/abs/2304.08485)   |
|      |         | GPT4-V OpenFlamingo            | [awadalla2023openflamingo](https://arxiv.org/abs/2308.01390)|
|      |         | InternGPT               | [liu2023interngpt](https://arxiv.org/abs/2305.05662)  |
|      |         | PaLM                 | [chowdhery2022palm](https://arxiv.org/abs/2204.02311)  |
|      | Spatial Understanding   | Gpt-driver               | [mao2023gptdriver](https://arxiv.org/abs/2310.01415)  |
|      |         | Path planners              | [aghzal2023large](https://arxiv.org/abs/2310.03249)   |
|      | Visual Question Answering  | CogVLM                | [wang2023cogvlm](https://arxiv.org/abs/2311.03079)  |
|      |         | ViperGPT                | [sur√≠s2023vipergpt](https://arxiv.org/abs/2303.08128)  |
|      |         | VISPROG               | [gupta2022visual](https://arxiv.org/abs/2211.11559)   |
|      |         | MM-ReAct                | [yang2023mmreact](https://arxiv.org/abs/2303.11381)   |
|      |         | Chameleon               | [lu2023chameleon](https://arxiv.org/abs/2304.09842)   |
|      | Quantitive Analysis     |  GPT4Vis       | [wu2023gpt4vis](https://arxiv.org/abs/2311.15732)  |
|      | Survey Papers     | A Survey of Large Language Models         | [zhao2023survey](https://arxiv.org/abs/2303.18223)  |
| In-Context Learning  | Chain of Thought    | Chain of Thought              | [wei2023chainofthought](https://arxiv.org/abs/2201.11903) |
|      |         | Tree of Thought             | [yao2023tree](https://arxiv.org/abs/2305.10601)    |
|      |         | Multimodal-CoT              | [zhang2023multimodal](https://arxiv.org/abs/2302.00923)  |
|      |         | Auto-CoT                | [zhang2022automatic](https://arxiv.org/abs/2210.03493) |
|      |         | Verify-and-Edit                | [zhao2023verifyandedit](https://arxiv.org/abs/2305.03268) |
|      |         | Skeleton-of-Thought                | [ning2023skeletonofthought](https://arxiv.org/abs/2307.15337) |
|      |         | Rethinking with Retrieval                | [he2022rethinking](https://arxiv.org/abs/2301.00303) |
|      | Reasoning      | Self-Consistency              | [wang2023selfconsistency](https://arxiv.org/abs/2203.11171)|
|      |         | ReAct                | [yao2023react](https://arxiv.org/abs/2303.11366)   |
|      |         | Self-Refine              | [madaan2023selfrefine](https://arxiv.org/abs/2303.17651) |
|      |         | Plan-and-Solve              | [wang2023planandsolve](https://arxiv.org/abs/2305.04091) |
|      |         | PAL                | [gao2023pal](https://arxiv.org/abs/2211.10435)   |
|      |         | Reasoning via Planning            | [hao2023reasoning](https://arxiv.org/abs/2305.14992)  |
|      |         | Self-Ask                | [press2023measuring](https://arxiv.org/abs/2210.03350) |
|      |         | Least-to-Most Prompting           | [zhou2023leasttomost](https://arxiv.org/abs/2205.10625)  |
|      |         | Self-Polish              | [xi2023selfpolish](https://arxiv.org/abs/2305.14497)  |
|      |         | COMPLEXITY-CoT              | [fu2023complexitybased](https://arxiv.org/abs/2210.00720) |
|      |         | SuperICL                | [xu2023small](https://arxiv.org/abs/2305.08848)    |
|      |         | VisualCOMET                | [park2020visualcomet](https://arxiv.org/abs/2004.10796)    |
|      | Memory     | MemoryBank                | [zhong2023memorybank](https://arxiv.org/abs/2305.10250)   |
|      |      | ChatEval                | [chan2023chateval](https://arxiv.org/abs/2308.07201)   |
|      |      | Generative Agents                | [park2023generative](https://arxiv.org/abs/2304.03442)   |
|      | Planning     | SelfCheck                | [miao2023selfcheck](https://arxiv.org/abs/2308.00436)   |
|      | Automation     | APE                | [zhou2023large](https://arxiv.org/abs/2211.01910)   |
|      | Self-supervised     | Self-supervised ICL   | [chen2023sinc](https://arxiv.org/abs/2307.07742)   |
|      | Benchmark     |  BIG-Bench       | [srivastava2023imitation](https://arxiv.org/abs/2206.04615)  |
|      |     |  ARB       | [sawada2023arb](https://arxiv.org/abs/2307.13692)  |
|      |     |  PlanBench       | [valmeekam2023planbench](https://arxiv.org/abs/2206.10498)  |
|      |     |  Chain-of-Thought Hub       | [fu2023chainofthought](https://arxiv.org/abs/2305.17306)  |
|      | Survey Paper     | A Survey of Chain of Thought Reasoning   | [chu2023survey](https://arxiv.org/abs/2309.15402)   |
| LLM for Agent  | Planning      | Voyager               | [wang2023voyager](https://arxiv.org/abs/2305.16291)   |
|      |         | DEPS                 | [wang2023describe](https://arxiv.org/abs/2302.01560)  |
|      |         | JARVIS-1                | [wang2023jarvis1](https://arxiv.org/abs/2311.05997)   |
|      |         | LLM+P              | [liu2023llmp](https://arxiv.org/abs/2304.11477)   |
|      |         | Autonomous Agents             | [wang2023survey](https://arxiv.org/abs/2308.11432)  |
|      | Reinforcement Learning  | Eureka                | [ma2023eureka](https://arxiv.org/abs/2310.12931)   |
|      |         | Language to Rewards            | [yu2023language](https://arxiv.org/abs/2306.08647)  |
|      |         | Language Instructed Reinforcement Learning       | [hu2023language](https://arxiv.org/abs/2304.07297)  |
|      |         | Lafite-RL               | [chu2023accelerating](https://arxiv.org/abs/2311.02379)  |
|      | Open-Source Evaluation  | AgentSims                | [lin2023agentsims](https://arxiv.org/abs/2308.04026)   |
|      | Survey Paper     | The Rise and Potential of Large Language Models     | [xi2023rise](https://arxiv.org/abs/2309.07864)   |
| LLM for Robots     | Multimodal prompts    | VIMA              | [jiang2023vima](https://arxiv.org/abs/2210.03094)   |
|        |        | Instruct2Act           | [huang2023instruct2act](https://arxiv.org/abs/2305.11176) |
|        |        | MOMA-Force            | [yang2023momaforce](https://arxiv.org/abs/2308.03624)  |
|        | Multimodal LLM     | PaLM-E             | [driess2023palme](https://arxiv.org/abs/2303.03378)  |
|        |        | GATO              | [reed2022generalist](https://arxiv.org/abs/2205.06175)  |
|        |        | Flamingo             | [alayrac2022flamingo](https://arxiv.org/abs/2204.14198) |
|        |        | Physically Grounded Vision-Language Model    | [gao2023physically](https://arxiv.org/abs/2309.02561)  |
|        |        | MOO               | [stone2023openworld](https://arxiv.org/abs/2303.00905)  |
|        | Code generation    | Code as policies           | [liang2023code](https://arxiv.org/abs/2209.07753)   |
|        |        | Progprompt            | [singh2022progprompt](https://arxiv.org/abs/2209.11302) |
|        |        | Socratic             | [zeng2022socratic](https://arxiv.org/abs/2204.00598)   |
|        |        | SMART-LLM             | [kannan2023smartllm](https://arxiv.org/abs/2309.10062)  |
|        |        | Statler              | [yoneda2023statler](https://arxiv.org/abs/2306.17840)  |
|        | Decomposing task     | SayCan             | [ahn2022i](https://arxiv.org/abs/2204.01691)     |
|        |        | Language Models as Zero-Shot Planners      | [huang2022language](https://arxiv.org/abs/2201.07207)  |
|        |        | SayPlan              | [rana2023sayplan](https://arxiv.org/abs/2307.06135)  |
|        |        | DOREMI             | [guo2023doremi](https://arxiv.org/abs/2307.00329)   |
|        | Low-level output     | SayTap             | [tang2023saytap](https://arxiv.org/abs/2306.07580)   |
|        |        | Prompt a Robot to Walk         | [wang2023prompt](https://arxiv.org/abs/2309.09969)   |
|        | Multimodal Data injection  | 3D-LLM             | [hong20233dllm](https://arxiv.org/abs/2307.12981)   |
|        |        | LiDAR-LLM             | [yang2023lidarllm](https://arxiv.org/abs/2312.14074)   |
|        | Data generation    | Gensim             | [wang2023gensim](https://arxiv.org/abs/2310.01361)   |
|        | Planning       | Embodied Task Planning         | [wu2023embodied](https://arxiv.org/abs/2307.01848)   |
|        | Self-improvement     | REFLECT              | [liu2023reflect](https://arxiv.org/abs/2306.15724)   |
|        |        | Reflexion             | [shinn2023reflexion](https://arxiv.org/abs/2303.11366)  |
|        | Chain of Thought     | EmbodiedGPT             | [mu2023embodiedgpt](https://arxiv.org/abs/2305.15021)  |
|        | Survey papers     | Toward General-Purpose         | [hu2023generalpurpose](https://arxiv.org/abs/2312.08782)  |
|        |        | Language-conditioned         | [zhou2023languageconditioned](https://arxiv.org/abs/2312.10807)|
|        |        | Foundation Models         | [firoozi2023foundation](https://arxiv.org/abs/2312.07843) |
|        |        | Robot Learning           | [xiao2023robot](https://arxiv.org/abs/2311.14379)   |
|        |        | The Development of LLMs          | [lin2023development](https://arxiv.org/abs/2311.00530)  |
| Perception     | Object Detection    | OWL-ViT              | [minderer2022simple](https://arxiv.org/abs/2205.06230)   |
|      |     | GLIP              | [li2022grounded](https://arxiv.org/abs/2112.03857)   |
|      |     | Grounding DINO              | [liu2023grounding](https://arxiv.org/abs/2303.05499)   |
|      |     | PointCLIP              | [zhang2021pointclip](https://arxiv.org/abs/2112.02413)   |
|      |     | Segment Anything              | [kirillov2023segment](https://arxiv.org/abs/2304.02643)   |
