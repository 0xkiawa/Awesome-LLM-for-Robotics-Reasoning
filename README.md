# Model Overview

|   No. | Category            | Subcategory               | Models                                     | Title                                                                                                                                                        |
|------:|:--------------------|:--------------------------|:-------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------|
|     1 | LLM                 | Open sourced LLM          | BERT                                       | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)                                         |
|     2 |                     |                           | T5                                         | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)                                        |
|     3 |                     |                           | LLaMA                                      | [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)                                                                     |
|     4 |                     |                           | OpenFlamingo                               | [OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models](https://arxiv.org/abs/2308.01390)                          |
|     5 |                     |                           | InstructBLIP                               | [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500)                                     |
|     6 |                     |                           | ChatBridge                                 | [ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst](https://arxiv.org/abs/2305.16103)                                         |
|     7 |                     | Closed sourced LLM        | GPT3                                       | [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)                                                                                    |
|     8 |                     |                           | GPT4                                       | [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)                                                                                                   |
|     9 |                     | Instruction Turning       | InstructGPT                                | [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)                                                      |
|    10 |                     |                           | LLaVA                                      | [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)                                                                                                |
|    11 |                     |                           | MiniGPT-4                                  | [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/abs/2304.10592)                                   |
|    12 |                     |                           | FLAN                                       | [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)                                                                         |
|    13 |                     |                           | LLaMA-adapter                              | [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/abs/2303.16199)                                         |
|    14 |                     |                           | Self-Instruct                              | [Self-Instruct: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/abs/2212.10560)                                                 |
|    15 |                     | Vision-LLM                | LLaVA                                      | [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)                                                                                                |
|    16 |                     |                           | GPT4-V OpenFlamingo                        | [OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models](https://arxiv.org/abs/2308.01390)                          |
|    17 |                     |                           | InternGPT                                  | [InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language](https://arxiv.org/abs/2305.05662)                                      |
|    18 |                     |                           | PaLM                                       | [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)                                                                            |
|    19 |                     | Spatial Understanding     | Gpt-driver                                 | [GPT-Driver: Learning to Drive with GPT](https://arxiv.org/abs/2310.01415)                                                                                   |
|    20 |                     |                           | Path planners                              | [Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning](https://arxiv.org/abs/2310.03249)             |
|    21 |                     | Visual Question Answering | CogVLM                                     | [CogVLM: Visual Expert for Pretrained Language Models](https://arxiv.org/abs/2311.03079)                                                                     |
|    22 |                     |                           | ViperGPT                                   | [ViperGPT: Visual Inference via Python Execution for Reasoning](https://arxiv.org/abs/2303.08128)                                                            |
|    23 |                     |                           | VISPROG                                    | [Visual Programming: Compositional visual reasoning without training](https://arxiv.org/abs/2211.11559)                                                      |
|    24 |                     |                           | MM-ReAct                                   | [MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action](https://arxiv.org/abs/2303.11381)                                                          |
|    25 |                     |                           | Chameleon                                  | [Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models](https://arxiv.org/abs/2304.09842)                                              |
|    26 |                     |                           | Caption Anything                           | [Caption Anything: Interactive Image Description with Diverse Multimodal Controls](https://arxiv.org/abs/2305.02677)                                         |
|    27 |                     | Temporal Logics           | NL2TL                                      | [NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models](https://arxiv.org/abs/2305.07766)                                     |
|    28 |                     | Quantitive Analysis       | GPT4Vis                                    | [GPT4Vis: What Can GPT-4 Do for Zero-shot Visual Recognition?](https://arxiv.org/abs/2311.15732)                                                             |
|    29 |                     |                           | Gemini vs GPT-4V                           | [Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases](https://arxiv.org/abs/2312.15011)           |
|    30 |                     | Survey Papers             | A Survey of Large Language Models          | [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)                                                                                        |
|    31 | In-Context Learning | Chain of Thought          | Chain of Thought                           | [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)                                                    |
|    32 |                     |                           | Tree of Thought                            | [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)                                                  |
|    33 |                     |                           | Multimodal-CoT                             | [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923)                                                                 |
|    34 |                     |                           | Auto-CoT                                   | [Automatic Chain of Thought Prompting in Large Language Models](https://arxiv.org/abs/2210.03493)                                                            |
|    35 |                     |                           | Verify-and-Edit                            | [Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework](https://arxiv.org/abs/2305.03268)                                                         |
|    36 |                     |                           | Skeleton-of-Thought                        | [Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding](https://arxiv.org/abs/2307.15337)                                                      |
|    37 |                     |                           | Rethinking with Retrieval                  | [Rethinking with Retrieval: Faithful Large Language Model Inference](https://arxiv.org/abs/2301.00303)                                                       |
|    38 |                     | Reasoning                 | Self-Consistency                           | [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)                                                  |
|    39 |                     |                           | ReAct                                      | [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2303.11366)                                                               |
|    40 |                     |                           | Self-Refine                                | [Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/abs/2303.17651)                                                                     |
|    41 |                     |                           | Plan-and-Solve                             | [Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models](https://arxiv.org/abs/2305.04091)                        |
|    42 |                     |                           | PAL                                        | [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435)                                                                                       |
|    43 |                     |                           | Reasoning via Planning                     | [Reasoning with Language Model is Planning with World Model](https://arxiv.org/abs/2305.14992)                                                               |
|    44 |                     |                           | Self-Ask                                   | [Measuring and Narrowing the Compositionality Gap in Language Models](https://arxiv.org/abs/2210.03350)                                                      |
|    45 |                     |                           | Least-to-Most Prompting                    | [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)                                               |
|    46 |                     |                           | Self-Polish                                | [Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement](https://arxiv.org/abs/2305.14497)                                           |
|    47 |                     |                           | COMPLEXITY-CoT                             | [Complexity-Based Prompting for Multi-Step Reasoning](https://arxiv.org/abs/2210.00720)                                                                      |
|    48 |                     |                           | SuperICL                                   | [Small Models are Valuable Plug-ins for Large Language Models](https://arxiv.org/abs/2305.08848)                                                             |
|    49 |                     |                           | VisualCOMET                                | [VisualCOMET: Reasoning about the Dynamic Context of a Still Image](https://arxiv.org/abs/2004.10796)                                                        |
|    50 |                     | Memory                    | MemoryBank                                 | [MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://arxiv.org/abs/2305.10250)                                                        |
|    51 |                     |                           | ChatEval                                   | [ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate](https://arxiv.org/abs/2308.07201)                                                 |
|    52 |                     |                           | Generative Agents                          | [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442)                                                               |
|    53 |                     | Planning                  | SelfCheck                                  | [SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning](https://arxiv.org/abs/2308.00436)                                                |
|    54 |                     | Automation                | APE                                        | [Large Language Models Are Human-Level Prompt Engineers](https://arxiv.org/abs/2211.01910)                                                                   |
|    55 |                     | Self-supervised           | Self-supervised ICL                        | [SINC: Self-Supervised In-Context Learning for Vision-Language Tasks](https://arxiv.org/abs/2307.07742)                                                      |
|    56 |                     | Benchmark                 | BIG-Bench                                  | [Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models](https://arxiv.org/abs/2206.04615)                             |
|    57 |                     |                           | ARB                                        | [ARB: Advanced Reasoning Benchmark for Large Language Models](https://arxiv.org/abs/2307.13692)                                                              |
|    58 |                     |                           | PlanBench                                  | [PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change](https://arxiv.org/abs/2206.10498)           |
|    59 |                     |                           | Chain-of-Thought Hub                       | [Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance](https://arxiv.org/abs/2305.17306)                        |
|    60 |                     | Survey Paper              | A Survey of Chain of Thought Reasoning     | [A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future](https://arxiv.org/abs/2309.15402)                                                   |
|    61 |                     |                           | Reasoning in Large Language Models         | [Towards Reasoning in Large Language Models: A Survey](https://arxiv.org/abs/2212.10403)                                                                     |
|    62 | LLM for Agent       |                           | Voyager                                    | [Voyager: An Open-Ended Embodied Agent with Large Language Models](https://arxiv.org/abs/2305.16291)                                                         |
|    63 |                     |                           | DEPS                                       | [Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents](https://arxiv.org/abs/2302.01560) |
|    64 |                     |                           | JARVIS-1                                   | [JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models](https://arxiv.org/abs/2311.05997)                                  |
|    65 |                     |                           | LLM+P                                      | [LLM+P: Empowering Large Language Models with Optimal Planning Proficiency](https://arxiv.org/abs/2304.11477)                                                |
|    66 |                     |                           | Autonomous Agents                          | [A Survey on Large Language Model based Autonomous Agents](https://arxiv.org/abs/2308.11432)                                                                 |
|    67 |                     |                           | AgentInstruct                              | [Agent Instructs Large Language Models to be General Zero-Shot Reasoners](https://arxiv.org/abs/2310.03710)                                                  |
|    68 |                     | Reinforcement Learning    | Eureka                                     | [Eureka: Human-Level Reward Design via Coding Large Language Models](https://arxiv.org/abs/2310.12931)                                                       |
|    69 |                     |                           | Language to Rewards                        | [Language to Rewards for Robotic Skill Synthesis](https://arxiv.org/abs/2306.08647)                                                                          |
|    70 |                     |                           | Language Instructed Reinforcement Learning | [Language Instructed Reinforcement Learning for Human-AI Coordination](https://arxiv.org/abs/2304.07297)                                                     |
|    71 |                     |                           | Lafite-RL                                  | [Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models](https://arxiv.org/abs/2311.02379)                     |
|    72 |                     |                           | ELLM                                       | [Guiding Pretraining in Reinforcement Learning with Large Language Models](https://openreview.net/forum?id=3s4fZTr1ce)                                                 |
|    73 |                     |                           | RLAdapter                                  | [RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds](https://arxiv.org/abs/2302.06692)                                                 |
|    74 |                     |                           | AdaRefiner                                 | [AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback](https://arxiv.org/abs/2309.17176)                                                 |
|    75 |                     |                           | Reward Design with Language Models         | [Reward Design with Languge Models](https://arxiv.org/abs/2303.00001)                                             |
|    76 |                     |                           | EAGER                                      | [EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL](https://arxiv.org/abs/2206.09674)                                 |
|    77 |                     | Open-Source Evaluation    | AgentSims                                  | [AgentSims: An Open-Source Sandbox for Large Language Model Evaluation](https://arxiv.org/abs/2308.04026)                                                    |
|    78 |                     |                           | Large Language Model Based Agents          | [The Rise and Potential of Large Language Model Based Agents: A Survey](https://arxiv.org/abs/2309.07864)                                                    |
|    79 | LLM for Robots      | Multimodal prompts        | VIMA                                       | [VIMA: General Robot Manipulation with Multimodal Prompts](https://arxiv.org/abs/2210.03094)                                                                 |
|    80 |                     |                           | Instruct2Act                               | [Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model](https://arxiv.org/abs/2305.11176)                           |
|    81 |                     |                           | MOMA-Force                                 | [MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation](https://arxiv.org/abs/2308.03624)                                                    |
|    82 |                     | Multimodal LLM            | PaLM-E                                     | [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378)                                                                            |
|    83 |                     |                           | GATO                                       | [A Generalist Agent](https://arxiv.org/abs/2205.06175)                                                                                                       |
|    84 |                     |                           | Flamingo                                   | [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198)                                                                  |
|    85 |                     |                           | Physically Grounded Vision-Language Model  | [Physically Grounded Vision-Language Models for Robotic Manipulation](https://arxiv.org/abs/2309.02561)                                                      |
|    86 |                     |                           | MOO                                        | [Open-World Object Manipulation using Pre-trained Vision-Language Models](https://arxiv.org/abs/2303.00905)                                                  |
|    87 |                     | Code generation           | Code as policies                           | [Code as Policies: Language Model Programs for Embodied Control](https://arxiv.org/abs/2209.07753)                                                           |
|    88 |                     |                           | Progprompt                                 | [ProgPrompt: Generating Situated Robot Task Plans using Large Language Models](https://arxiv.org/abs/2209.11302)                                             |
|    89 |                     |                           | Socratic                                   | [Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598)                                                  |
|    90 |                     |                           | SMART-LLM                                  | [SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models](https://arxiv.org/abs/2309.10062)                                             |
|    91 |                     |                           | Statler                                    | [Statler: State-Maintaining Language Models for Embodied Reasoning](https://arxiv.org/abs/2306.17840)                                                        |
|    92 |                     | Decomposing task          | SayCan                                     | [Do As I Can, Not As I Say: Grounding Language in Robotic Affordances](https://arxiv.org/abs/2204.01691)                                                     |
|    93 |                     |                           | Language Models as Zero-Shot Planners      | [Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents](https://arxiv.org/abs/2201.07207)                               |
|    94 |                     |                           | SayPlan                                    | [SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning](https://arxiv.org/abs/2307.06135)                          |
|    95 |                     |                           | DOREMI                                     | [DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment](https://arxiv.org/abs/2307.00329)                            |
|    96 |                     | Low-level output          | SayTap                                     | [SayTap: Language to Quadrupedal Locomotion](https://arxiv.org/abs/2306.07580)                                                                               |
|    97 |                     |                           | Prompt a Robot to Walk                     | [Prompt a Robot to Walk with Large Language Models](https://arxiv.org/abs/2309.09969)                                                                        |
|    98 |                     | Multimodal Data injection | 3D-LLM                                     | [3D-LLM: Injecting the 3D World into Large Language Models](https://arxiv.org/abs/2307.12981)                                                                |
|    99 |                     |                           | LiDAR-LLM                                  | [LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding](https://arxiv.org/abs/2312.14074)                                   |
|   100 |                     | Data generation           | Gensim                                     | [GenSim: Generating Robotic Simulation Tasks via Large Language Models](https://arxiv.org/abs/2310.01361)                                                    |
|   101 |                     |                           | RoboGen                                    | [RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation](https://arxiv.org/abs/2311.01455)                         |
|   102 |                     |                           | Embodied Task Planning                     | [Embodied Task Planning with Large Language Models](https://arxiv.org/abs/2307.01848)                                                                        |
|   103 |                     | Self-improvement          | REFLECT                                    | [REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction](https://arxiv.org/abs/2306.15724)                                            |
|   104 |                     |                           | Reflexion                                  | [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)                                                            |
|   105 |                     |                           | EmbodiedGPT                                | [EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought](https://arxiv.org/abs/2305.15021)                                                  |
|   106 |                     | Survey papers             | Toward General-Purpose                     | [Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis](https://arxiv.org/abs/2312.08782)                                          |
|   107 |                     |                           | Language-conditioned                       | [Language-conditioned Learning for Robotic Manipulation: A Survey](https://arxiv.org/abs/2312.10807)                                                         |
|   108 |                     |                           | Foundation Models                          | [Foundation Models in Robotics: Applications, Challenges, and the Future](https://arxiv.org/abs/2312.07843)                                                  |
|   109 |                     |                           | Robot Learning                             | [Robot Learning in the Era of Foundation Models: A Survey](https://arxiv.org/abs/2311.14379)                                                                 |
|   110 |                     |                           | The Development of LLMs                    | [The Development of LLMs for Embodied Navigation](https://arxiv.org/abs/2311.00530)                                                                          |
|   111 | Perception          | Object Detection          | OWL-ViT                                    | [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230)                                                         |
|   112 |                     |                           | GLIP                                       | [Grounded Language-Image Pre-training](https://arxiv.org/abs/2112.03857)                                                                                     |
|   113 |                     |                           | Grounding DINO                             | [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499)                                   |
|   114 |                     |                           | PointCLIP                                  | [PointCLIP: Point Cloud Understanding by CLIP](https://arxiv.org/abs/2112.02413)                                                                             |
|   115 |                     |                           | Segment Anything                           | [Segment Anything](https://arxiv.org/abs/2304.02643)                                                                                                         |
